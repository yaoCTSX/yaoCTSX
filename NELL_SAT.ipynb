{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yaoCTSX/yaoCTSX/blob/main/NELL_SAT.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8V1RQTvJ2IoN",
        "outputId": "b0326d9c-02ae-42e1-9aa0-6024b6a6d401"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Looking in links: https://data.pyg.org/whl/torch-1.13.0+cpu.html\n",
            "Collecting pyg_lib\n",
            "  Downloading https://data.pyg.org/whl/torch-1.13.0%2Bcpu/pyg_lib-0.2.0%2Bpt113cpu-cp39-cp39-linux_x86_64.whl (626 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m626.9/626.9 KB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting torch_scatter\n",
            "  Downloading https://data.pyg.org/whl/torch-1.13.0%2Bcpu/torch_scatter-2.1.1%2Bpt113cpu-cp39-cp39-linux_x86_64.whl (485 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m485.7/485.7 KB\u001b[0m \u001b[31m11.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting torch_sparse\n",
            "  Downloading https://data.pyg.org/whl/torch-1.13.0%2Bcpu/torch_sparse-0.6.17%2Bpt113cpu-cp39-cp39-linux_x86_64.whl (1.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting torch_cluster\n",
            "  Downloading https://data.pyg.org/whl/torch-1.13.0%2Bcpu/torch_cluster-1.6.1%2Bpt113cpu-cp39-cp39-linux_x86_64.whl (700 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m700.6/700.6 KB\u001b[0m \u001b[31m13.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting torch_spline_conv\n",
            "  Downloading https://data.pyg.org/whl/torch-1.13.0%2Bcpu/torch_spline_conv-1.2.2%2Bpt113cpu-cp39-cp39-linux_x86_64.whl (198 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m198.9/198.9 KB\u001b[0m \u001b[31m9.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting torch_geometric\n",
            "  Downloading torch_geometric-2.3.0.tar.gz (616 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m616.2/616.2 KB\u001b[0m \u001b[31m17.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.9/dist-packages (from torch_sparse) (1.10.1)\n",
            "Requirement already satisfied: pyparsing in /usr/local/lib/python3.9/dist-packages (from torch_geometric) (3.0.9)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.9/dist-packages (from torch_geometric) (3.1.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.9/dist-packages (from torch_geometric) (1.22.4)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.9/dist-packages (from torch_geometric) (2.27.1)\n",
            "Requirement already satisfied: psutil>=5.8.0 in /usr/local/lib/python3.9/dist-packages (from torch_geometric) (5.9.4)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.9/dist-packages (from torch_geometric) (1.2.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.9/dist-packages (from torch_geometric) (4.65.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.9/dist-packages (from jinja2->torch_geometric) (2.1.2)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests->torch_geometric) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests->torch_geometric) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests->torch_geometric) (3.4)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests->torch_geometric) (1.26.15)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.9/dist-packages (from scikit-learn->torch_geometric) (3.1.0)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.9/dist-packages (from scikit-learn->torch_geometric) (1.1.1)\n",
            "Building wheels for collected packages: torch_geometric\n",
            "  Building wheel for torch_geometric (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for torch_geometric: filename=torch_geometric-2.3.0-py3-none-any.whl size=909897 sha256=710e345424940c8dc0b471eae17bdcb7bf3c47f0d9756fa24b1f60c842bf6c90\n",
            "  Stored in directory: /root/.cache/pip/wheels/cd/7d/6b/17150450b80b4a3656a84330e22709ccd8dc0f8f4773ba4133\n",
            "Successfully built torch_geometric\n",
            "Installing collected packages: torch_spline_conv, torch_scatter, pyg_lib, torch_sparse, torch_cluster, torch_geometric\n",
            "Successfully installed pyg_lib-0.2.0+pt113cpu torch_cluster-1.6.1+pt113cpu torch_geometric-2.3.0 torch_scatter-2.1.1+pt113cpu torch_sparse-0.6.17+pt113cpu torch_spline_conv-1.2.2+pt113cpu\n"
          ]
        }
      ],
      "source": [
        "!pip install pyg_lib torch_scatter torch_sparse torch_cluster torch_spline_conv torch_geometric -f https://data.pyg.org/whl/torch-1.13.0+cpu.html"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eqLpHnTebuL8"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oufJGGh863hX"
      },
      "outputs": [],
      "source": [
        "import torch_geometric"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pUDvR3WU2IlJ"
      },
      "outputs": [],
      "source": [
        "#utils\n",
        "import torch\n",
        "\n",
        "\n",
        "def sparse_to_tensor(matrix):\n",
        "    \"Converts a sparse matrix to a 3 x N matrix\"\n",
        "    indices = matrix.coalesce().indices()\n",
        "    values = matrix.coalesce().values().unsqueeze(0)\n",
        "    return torch.cat([indices, values], dim=0)\n",
        "\n",
        "\n",
        "def tensor_to_sparse(matrix):\n",
        "    \"Converts a 3 x N matrix to a sparse matrix\"\n",
        "    indices = matrix[0:2]\n",
        "    values = matrix[2:3].squeeze()\n",
        "    return torch.sparse_coo_tensor(indices, values)\n",
        "\n",
        "\n",
        "def ensure_input_is_tensor(input):\n",
        "    if input.is_sparse:\n",
        "        input = sparse_to_tensor(input)\n",
        "    return input\n",
        "\n",
        "\n",
        "def edge_to_node_matrix(edges, nodes, one_indexed=True):\n",
        "    sigma1 = torch.zeros((len(nodes), len(edges)), dtype=torch.float)\n",
        "    offset = int(one_indexed)\n",
        "    j = 0\n",
        "    for edge in edges:\n",
        "        x, y = edge\n",
        "        sigma1[x - offset][j] -= 1\n",
        "        sigma1[y - offset][j] += 1\n",
        "        j += 1\n",
        "    return sigma1\n",
        "\n",
        "\n",
        "def triangle_to_edge_matrix(triangles, edges):\n",
        "    sigma2 = torch.zeros((len(edges), len(triangles)), dtype=torch.float)\n",
        "    edges = [e for e in edges]\n",
        "    edges = {edges[i]: i for i in range(len(edges))}\n",
        "    for l in range(len(triangles)):\n",
        "        i, j, k = triangles[l]\n",
        "        if (i, j) in edges:\n",
        "            sigma2[edges[(i, j)]][l] += 1\n",
        "        else:\n",
        "            sigma2[edges[(j, i)]][l] -= 1\n",
        "\n",
        "        if (j, k) in edges:\n",
        "            sigma2[edges[(j, k)]][l] += 1\n",
        "        else:\n",
        "            sigma2[edges[(k, j)]][l] -= 1\n",
        "\n",
        "        if (i, k) in edges:\n",
        "            sigma2[edges[(i, k)]][l] -= 1\n",
        "        else:\n",
        "            sigma2[edges[(k, i)]][l] += 1\n",
        "\n",
        "    return sigma2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z9LKrSN42qjI"
      },
      "outputs": [],
      "source": [
        "#cochain\n",
        "import torch\n",
        "#from utils import sparse_to_tensor #+\n",
        "\n",
        "def stl(t):\n",
        "    \"Shape to list\"\n",
        "    return list(t.shape)\n",
        "\n",
        "class CoChain:\n",
        "\n",
        "    def __init__(self, X0, X1, X2, b1, b2, label):\n",
        "        self.X0 = X0\n",
        "        self.X1 = X1\n",
        "        self.X2 = X2\n",
        "        # b1 and b2 can either be sparse or dense but since python doesn't really have overloading, doing this instead\n",
        "        if b1.is_sparse:\n",
        "            b1 = sparse_to_tensor(b1)\n",
        "        self.b1 = b1\n",
        "\n",
        "        if b2.is_sparse:\n",
        "            b2 = sparse_to_tensor(b2)\n",
        "        self.b2 = b2\n",
        "\n",
        "        self.label = label\n",
        "\n",
        "\n",
        "    def __str__(self):\n",
        "        name = f\"CoChain(X0={stl(self.X0)}, X1={stl(self.X1)}, X2={stl(self.X2)},\" \\\n",
        "               f\" b1={stl(self.b1)}, b2={stl(self.b2)}, label={stl(self.label)})\"\n",
        "        return name\n",
        "\n",
        "    def __repr__(self):\n",
        "        name = f\"CoChain(X0={stl(self.X0)}, X1={stl(self.X1)}, X2={stl(self.X2)},\" \\\n",
        "               f\" b1={stl(self.b1)}, b2={stl(self.b2)}, label={stl(self.label)})\"\n",
        "        return name\n",
        "\n",
        "    def __eq__(self, other):\n",
        "        x0 = torch.all(torch.eq(self.X0, other.X0)).item()\n",
        "        x1 = torch.all(torch.eq(self.X1, other.X1)).item()\n",
        "        x2 = torch.all(torch.eq(self.X2, other.X2)).item()\n",
        "        s1 = torch.all(torch.eq(self.b1, other.b1)).item()\n",
        "        s2 = torch.all(torch.eq(self.b2, other.b2)).item()\n",
        "        l0 = torch.all(torch.eq(self.label, other.label)).item()\n",
        "        return all([x0, x1, x2, s1, s2, l0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N0_Gr6jL2ql9"
      },
      "outputs": [],
      "source": [
        "#device\n",
        "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wvpbs_452qqB"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3ywXpCHn2qs1"
      },
      "outputs": [],
      "source": [
        "#simplicial_complex\n",
        "import torch\n",
        "#from utils import ensure_input_is_tensor #+\n",
        "#from constants import DEVICE #+\n",
        "\n",
        "\n",
        "class SimplicialComplex:\n",
        "\n",
        "    def __init__(self, X0, X1, X2, L0, L1, L2, label, batch=None):\n",
        "        self.X0 = X0\n",
        "        self.X1 = X1\n",
        "        self.X2 = X2\n",
        "\n",
        "        # L0, L1 and L2 can either be sparse or dense but since python doesn't really have overloading, doing this instead\n",
        "        self.L0 = ensure_input_is_tensor(L0)\n",
        "        if L1 is not None:\n",
        "            self.L1 = ensure_input_is_tensor(L1)\n",
        "        else:\n",
        "            self.L1 = L1\n",
        "        if L2 is not None:\n",
        "            self.L2 = ensure_input_is_tensor(L2)\n",
        "        else:\n",
        "            self.L2 = L2\n",
        "\n",
        "        self.label = label\n",
        "        self.batch = batch\n",
        "\n",
        "    def __eq__(self, other):\n",
        "        x0 = torch.allclose(self.X0, other.X0, atol=1e-5)\n",
        "        x1 = torch.allclose(self.X1, other.X1, atol=1e-5)\n",
        "        x2 = torch.allclose(self.X2, other.X2, atol=1e-5)\n",
        "        l0 = torch.allclose(self.L0, other.L0, atol=1e-5)\n",
        "        l1 = torch.allclose(self.L1, other.L1, atol=1e-5)\n",
        "        l2 = torch.allclose(self.L2, other.L2, atol=1e-5)\n",
        "        label = torch.allclose(self.label, other.label, atol=1e-5)\n",
        "        return all([x0, x1, x2, l0, l1, l2, label])\n",
        "\n",
        "    def unpack_features(self):\n",
        "        return self.X0, self.X1, self.X2\n",
        "\n",
        "    def unpack_laplacians(self):\n",
        "        return self.L0, self.L1, self.L2\n",
        "\n",
        "    def unpack_batch(self):\n",
        "        return self.batch\n",
        "\n",
        "    def to_device(self):\n",
        "        self.X0 = self.X0.to(DEVICE)\n",
        "        self.X1 = self.X1.to(DEVICE)\n",
        "        self.X2 = self.X2.to(DEVICE)\n",
        "\n",
        "        self.L0 = self.L0.to(DEVICE)\n",
        "        if self.L1 is not None:\n",
        "            self.L1 = self.L1.to(DEVICE)\n",
        "        if self.L2 is not None:\n",
        "            self.L2 = self.L2.to(DEVICE)\n",
        "\n",
        "        self.batch = [batch.to(DEVICE) for batch in self.batch]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wHppid7P2qvq"
      },
      "outputs": [],
      "source": [
        "#nn.utils\n",
        "import torch\n",
        "import scipy\n",
        "import scipy.sparse.linalg as spl\n",
        "import numpy as np\n",
        "from scipy.sparse import coo_matrix\n",
        "import networkx as nx\n",
        "#from utils import edge_to_node_matrix, triangle_to_edge_matrix #+\n",
        "#from models.CoChain import CoChain #+\n",
        "import functools\n",
        "import scipy.sparse as sp\n",
        "\n",
        "\n",
        "def normalise_boundary(b1, b2):\n",
        "    B1, B2 = to_sparse_coo(b1), to_sparse_coo(b2)\n",
        "    x0, x1 = B1.shape\n",
        "    _, x2 = B2.shape\n",
        "\n",
        "    B1_v_abs, B1_i = torch.abs(B1.coalesce().values()), B1.coalesce().indices()\n",
        "    B1_sum = torch.sparse.sum(torch.sparse_coo_tensor(B1_i, B1_v_abs, (x0, x1)), dim=1)\n",
        "    B1_sum_values = B1_sum.to_dense()\n",
        "    B1_sum_indices = torch.tensor([i for i in range(x0)])\n",
        "    d0_diag_indices = torch.stack([B1_sum_indices, B1_sum_indices], dim=0)\n",
        "    B1_sum_inv_values = torch.nan_to_num(1. / B1_sum_values, nan=0., posinf=0., neginf=0.)\n",
        "\n",
        "    D1_inv = torch.sparse_coo_tensor(d0_diag_indices, 0.5 * B1_sum_inv_values, (x0, x0))\n",
        "    D3_values = (1 / 3.) * torch.ones(B2.shape[1])\n",
        "    D3_indices = [i for i in range(B2.shape[1])]\n",
        "    D3_indices = torch.tensor([D3_indices, D3_indices])\n",
        "    D3 = torch.sparse_coo_tensor(D3_indices, D3_values, (x2, x2))\n",
        "\n",
        "    B2D3 = torch.sparse.mm(B2, D3)\n",
        "    D1invB1 = (1 / np.sqrt(2.)) * torch.sparse.mm(D1_inv, B1)\n",
        "\n",
        "    return D1invB1, B2D3\n",
        "\n",
        "\n",
        "def preprocess_features(features):\n",
        "    \"\"\"Row-normalize feature matrix\"\"\"\n",
        "    rowsum = np.array(features.sum(1))\n",
        "    r_inv = np.power(rowsum, -1).flatten()\n",
        "    r_inv[np.isinf(r_inv)] = 0.\n",
        "    r_mat_inv = sp.diags(r_inv)\n",
        "    features = r_mat_inv.dot(features)\n",
        "    return torch.tensor(features, dtype=torch.float)\n",
        "\n",
        "\n",
        "def remove_diag_sparse(sparse_adj):\n",
        "    scipy_adj = torch_sparse_to_scipy_sparse(sparse_adj)\n",
        "    scipy_adj = scipy.sparse.triu(scipy_adj, k=1)\n",
        "    return scipy_sparse_to_torch_sparse(scipy_adj)\n",
        "\n",
        "\n",
        "def get_features(features, sc_list):\n",
        "    def _get_features(features, sc):\n",
        "        f = [features[i] for i in sc]\n",
        "        return functools.reduce(lambda a, b: a + b, f).float()\n",
        "        # return functools.reduce(lambda a, b: torch.logical_and(a, b), f).float()\n",
        "        # return a/torch.sum(s)\n",
        "\n",
        "    features = [_get_features(features, sc) for sc in sc_list]\n",
        "    if bool(features):\n",
        "        return torch.stack(features, dim=0)\n",
        "    else:\n",
        "        return torch.tensor([])\n",
        "\n",
        "\n",
        "def filter_simplices(node_features, simplice):\n",
        "    s = [node_features[i] for i in simplice]\n",
        "    common_features = functools.reduce(lambda a, b: torch.logical_and(a, b), s).float()\n",
        "    return torch.sum(common_features).item() > 0\n",
        "\n",
        "\n",
        "def correct_orientation(L, up_or_down):\n",
        "    \"\"\"\n",
        "    L : n * n sparse Laplacian matrix\n",
        "    up_or_down : int in {-1, 1}\n",
        "    \"\"\"\n",
        "    # Add 2 to identity\n",
        "    identity = 2 * torch.ones(L.shape[0])\n",
        "    identity_indices = torch.arange(L.shape[0])\n",
        "    identity_indices = torch.stack([identity_indices, identity_indices], dim=0)\n",
        "    sparse_identity = torch.sparse_coo_tensor(identity_indices, identity)\n",
        "    adj = L + sparse_identity\n",
        "\n",
        "    indices = adj.coalesce().indices()\n",
        "    values = adj.coalesce().values() * up_or_down\n",
        "    values[values < -1] = 1\n",
        "    values = torch.sign(values)\n",
        "\n",
        "    return torch.sparse_coo_tensor(indices, values)\n",
        "\n",
        "\n",
        "def convert_to_CoChain(adj, features, labels, X1=None, X2=None):\n",
        "    X0 = features\n",
        "\n",
        "    nodes = [i for i in range(X0.shape[0])]\n",
        "    edges = adj.coalesce().indices().tolist()\n",
        "    edges = [(i, j) for i, j in zip(edges[0], edges[1])]\n",
        "    # edges = [*filter(lambda x: filter_simplices(features, x), edges)]\n",
        "\n",
        "    g = nx.Graph()\n",
        "    g.add_nodes_from(nodes)\n",
        "    g.add_edges_from(edges)\n",
        "    triangles = [list(sorted(x)) for x in nx.enumerate_all_cliques(g) if len(x) == 3]\n",
        "    # triangles = [*filter(lambda x: filter_simplices(features, x), triangles)]\n",
        "    b1 = edge_to_node_matrix(edges, nodes, one_indexed=False).to_sparse()\n",
        "    b2 = triangle_to_edge_matrix(triangles, edges).to_sparse()\n",
        "\n",
        "    if X1 is None:\n",
        "        X1 = torch.tensor(edges)\n",
        "\n",
        "    if X2 is None:\n",
        "        X2 = torch.tensor(triangles)\n",
        "\n",
        "    return CoChain(X0, X1, X2, b1, b2, labels)\n",
        "\n",
        "\n",
        "def repair_sparse(matrix, ideal_shape):\n",
        "    # Only use this if last few cols/rows are empty and were removed in sparse operation\n",
        "    i_x, i_y = ideal_shape\n",
        "    m_x, m_y = matrix.shape[0], matrix.shape[1]\n",
        "    indices = matrix.coalesce().indices()\n",
        "    values = matrix.coalesce().values()\n",
        "    if i_x > m_x or i_y > m_y:\n",
        "        additional_i = torch.tensor([[i_x - 1], [i_y - 1]], dtype=torch.float)\n",
        "        additional_v = torch.tensor([0], dtype=torch.float)\n",
        "        indices = torch.cat([indices, additional_i], dim=1)\n",
        "        values = torch.cat([values, additional_v], dim=0)\n",
        "    return torch.sparse_coo_tensor(indices, values)\n",
        "\n",
        "\n",
        "def to_sparse_coo(matrix):\n",
        "    indices = matrix[0:2]\n",
        "    values = matrix[2:3].squeeze()\n",
        "    return torch.sparse_coo_tensor(indices, values)\n",
        "\n",
        "\n",
        "def sparse_diag_identity(n):\n",
        "    i = [i for i in range(n)]\n",
        "    return torch.sparse_coo_tensor(torch.tensor([i, i]), torch.ones(n))\n",
        "\n",
        "\n",
        "def sparse_diag(tensor):\n",
        "    i = [i for i in range(tensor.shape[0])]\n",
        "    return torch.sparse_coo_tensor(torch.tensor([i, i]), tensor)\n",
        "\n",
        "\n",
        "def chebyshev(L, X, k=3):\n",
        "    if k == 1:\n",
        "        return torch.sparse.mm(L, X)\n",
        "    dp = [X, torch.sparse.mm(L, X)]\n",
        "    for i in range(2, k):\n",
        "        nxt = 2 * (torch.sparse.mm(L, dp[i - 1]))\n",
        "        dp.append(torch.sparse.FloatTensor.add(nxt, -(dp[i - 2])))\n",
        "    return torch.cat(dp, dim=1)\n",
        "\n",
        "\n",
        "def torch_sparse_to_scipy_sparse(matrix):\n",
        "    i = matrix.coalesce().indices().cpu()\n",
        "    v = matrix.coalesce().values().cpu()\n",
        "\n",
        "    (m, n) = matrix.shape[0], matrix.shape[1]\n",
        "    return coo_matrix((v, i), shape=(m, n))\n",
        "\n",
        "\n",
        "def scipy_sparse_to_torch_sparse(matrix):\n",
        "    values = matrix.data\n",
        "    indices = np.vstack((matrix.row, matrix.col))\n",
        "\n",
        "    i = torch.LongTensor(indices)\n",
        "    v = torch.FloatTensor(values)\n",
        "    return torch.sparse.FloatTensor(i, v)\n",
        "\n",
        "\n",
        "def normalise(L):\n",
        "    M = L.shape[0]\n",
        "    L = torch_sparse_to_scipy_sparse(L)\n",
        "    topeig = spl.eigsh(L, k=1, which=\"LM\", return_eigenvectors=False)[0]\n",
        "    ret = L.copy()\n",
        "    ret *= 2.0 / topeig\n",
        "    ret.setdiag(np.ones(M) - ret.diagonal(0), 0)\n",
        "    return scipy_sparse_to_torch_sparse(ret)\n",
        "\n",
        "\n",
        "def batch_all_feature_and_lapacian_pair(X, L_i, L_v):\n",
        "    X_batch, I_batch, V_batch, batch_index = [], [], [], []\n",
        "    for i in range(len(X)):\n",
        "        x, i, v, batch = batch_feature_and_lapacian_pair(X[i], L_i[i], L_v[i])\n",
        "        X_batch.append(x)\n",
        "        I_batch.append(i)\n",
        "        V_batch.append(v)\n",
        "        batch_index.append(batch)\n",
        "\n",
        "    features_dct = {'features': X_batch,\n",
        "                    'lapacian_indices': I_batch,\n",
        "                    'lapacian_values': V_batch,\n",
        "                    'batch_index': batch_index}\n",
        "\n",
        "    # I_batch and V_batch form the indices and values of coo_sparse tensor but sparse tensors\n",
        "    # cant be stored so storing them as two separate tensors\n",
        "    return features_dct\n",
        "\n",
        "\n",
        "def batch_feature_and_lapacian_pair(x_list, L_i_list, L_v_list):\n",
        "    feature_batch = torch.cat(x_list, dim=0)\n",
        "    sizes = [*map(lambda x: x.size()[0], x_list)]\n",
        "\n",
        "    I_cat, V_cat = batch_sparse_matrix(L_i_list, L_v_list, sizes, sizes)\n",
        "    batch = [[i for _ in range(sizes[i])] for i in range(len(sizes))]\n",
        "    batch = torch.tensor([i for sublist in batch for i in sublist])\n",
        "    return feature_batch, I_cat, V_cat, batch\n",
        "\n",
        "\n",
        "def batch_sparse_matrix(L_i_list, L_v_list, size_x, size_y):\n",
        "    L_i_list = list(L_i_list)\n",
        "    mx_x, mx_y = 0, 0\n",
        "    for i in range(1, len(L_i_list)):\n",
        "        mx_x += size_x[i - 1]\n",
        "        mx_y += size_y[i - 1]\n",
        "        L_i_list[i][0] += mx_x\n",
        "        L_i_list[i][1] += mx_y\n",
        "    I_cat = torch.cat(L_i_list, dim=1)\n",
        "    V_cat = torch.cat(L_v_list, dim=0)\n",
        "    return I_cat, V_cat"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w5FTASm92qyV"
      },
      "outputs": [],
      "source": [
        "#processor_template\n",
        "from abc import abstractmethod, ABC\n",
        "#from models.SimplicialComplex import SimplicialComplex #+\n",
        "\n",
        "class NNProcessor(ABC):\n",
        "\n",
        "\t@abstractmethod\n",
        "\tdef process(self, CoChain):\n",
        "\t\t# Given a CoChain object, continue to process it until the structure can be stored in inmemorydataset\n",
        "\t\tpass\n",
        "\n",
        "\t@abstractmethod\n",
        "\tdef collate(self, objectList: list):\n",
        "\t\t# Given a list of objects which we have chosen to represent out dataset, combine into one big object to write to memory\n",
        "\t\tpass\n",
        "\n",
        "\t@abstractmethod\n",
        "\tdef get(self, data: SimplicialComplex, slice : dict, idx : int):\n",
        "\t\t# Given an index and a collated object, take out the individual object\n",
        "\t\tpass\n",
        "\n",
        "\t@abstractmethod\n",
        "\tdef batch(self, objectList: list):\n",
        "\t\t# Given a list of objects which are representations how we want to store data for each model, batch it and\n",
        "\t\t# store the fields in a feature_dct.\n",
        "\t\t# returns feature_dct, label which is a dictionary, torch.tensor\n",
        "\t\tpass\n",
        "\n",
        "\t@abstractmethod\n",
        "\tdef clean_features(self, simplicialComplex: SimplicialComplex):\n",
        "\t\t# Torch sparse matrix cannot be used during multiprocessing. One way of getting past that is storing the\n",
        "\t\t# indices and values as separate tensors and combining them again when single threaded. This is done in this function\n",
        "\t\tpass\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RC9O3WoI2IiL"
      },
      "outputs": [],
      "source": [
        "#sat processor\n",
        "import torch\n",
        "#from models.ProcessorTemplate import NNProcessor #+\n",
        "#from utils import ensure_input_is_tensor #+\n",
        "#from models.nn_utils import to_sparse_coo #+\n",
        "#from models.nn_utils import batch_all_feature_and_lapacian_pair, correct_orientation #+\n",
        "#from models.SimplicialComplex import SimplicialComplex #+\n",
        "#from constants import DEVICE\n",
        "\n",
        "\n",
        "class SATComplex(SimplicialComplex):\n",
        "\n",
        "    def __init__(self, X0, X1, X2, L0, L1_up, L1_down, L2, label, batch=None):\n",
        "        super().__init__(X0, X1, X2, L0, None, L2, label, batch=batch)\n",
        "\n",
        "        self.L1_up = ensure_input_is_tensor(L1_up)\n",
        "        self.L1_down = ensure_input_is_tensor(L1_down)\n",
        "\n",
        "    def __eq__(self, other):\n",
        "        x0 = torch.allclose(self.X0, other.X0, atol=1e-5)\n",
        "        x1 = torch.allclose(self.X1, other.X1, atol=1e-5)\n",
        "        x2 = torch.allclose(self.X2, other.X2, atol=1e-5)\n",
        "        l0 = torch.allclose(self.L0, other.L0, atol=1e-5)\n",
        "        l1_u = torch.allclose(self.L1_up, other.L1_up, atol=1e-5)\n",
        "        l1_d = torch.allclose(self.L1_down, other.L1_down, atol=1e-5)\n",
        "        l2 = torch.allclose(self.L2, other.L2, atol=1e-5)\n",
        "        label = torch.allclose(self.label, other.label, atol=1e-5)\n",
        "        return all([x0, x1, x2, l0, l1_u, l1_d, l2, label])\n",
        "\n",
        "    def unpack_up_down(self):\n",
        "        return [self.L1_up, self.L1_down]\n",
        "\n",
        "    def to_device(self):\n",
        "        super().to_device()\n",
        "        self.L1_up = self.L1_up.to(DEVICE)\n",
        "        self.L1_down = self.L1_down.to(DEVICE)\n",
        "\n",
        "\n",
        "class SATProcessor(NNProcessor):\n",
        "\n",
        "    def process(self, CoChain):\n",
        "        b1, b2 = to_sparse_coo(CoChain.b1), to_sparse_coo(CoChain.b2)\n",
        "\n",
        "        X0, X1, X2 = CoChain.X0, CoChain.X1, CoChain.X2\n",
        "\n",
        "        L0 = torch.sparse.mm(b1, b1.t())\n",
        "        L1_up = torch.sparse.mm(b2, b2.t())\n",
        "        L1_down = torch.sparse.mm(b1.t(), b1)\n",
        "        L2 = torch.sparse.mm(b2.t(), b2)\n",
        "\n",
        "        L0 = correct_orientation(L0, 1)\n",
        "        L1_up = correct_orientation(L1_up, 1)\n",
        "        L1_down = correct_orientation(L1_down, 1)\n",
        "        L2 = correct_orientation(L2, 1)\n",
        "\n",
        "        assert (X0.shape[0] == L0.shape[0])\n",
        "        assert (X1.shape[0] == L1_up.shape[0])\n",
        "        assert (X1.shape[0] == L1_down.shape[0])\n",
        "        assert (X2.shape[0] == L2.shape[0])\n",
        "\n",
        "        label = CoChain.label\n",
        "\n",
        "        return SATComplex(X0, X1, X2, L0, L1_up, L1_down, L2, label)\n",
        "\n",
        "    def collate(self, data_list):\n",
        "        X0, X1, X2 = [], [], []\n",
        "        L0, L1_up, L1_dn, L2 = [], [], [], []\n",
        "        label = []\n",
        "\n",
        "        x0_total, x1_total, x2_total = 0, 0, 0\n",
        "        l0_total, l1_u_total, l1_d_total, l2_total = 0, 0, 0, 0\n",
        "        label_total = 0\n",
        "\n",
        "        slices = {\"X0\": [0],\n",
        "                  \"X1\": [0],\n",
        "                  \"X2\": [0],\n",
        "                  \"L0\": [0],\n",
        "                  \"L1_up\": [0],\n",
        "                  \"L1_down\": [0],\n",
        "                  \"L2\": [0],\n",
        "                  \"label\": [0]}\n",
        "\n",
        "        for data in data_list:\n",
        "            x0, x1, x2 = data.X0, data.X1, data.X2\n",
        "            l0, l1_up, l1_dn, l2 = data.L0, data.L1_up, data.L1_down, data.L2\n",
        "            l = data.label\n",
        "\n",
        "            x0_s, x1_s, x2_s = x0.shape[0], x1.shape[0], x2.shape[0]\n",
        "            l0_s, l1_u_s, l1_d_s, l2_s = l0.shape[1], l1_up.shape[1], l1_dn.shape[1], l2.shape[1]\n",
        "            l_s = l.shape[0]\n",
        "\n",
        "            X0.append(x0)\n",
        "            X1.append(x1)\n",
        "            X2.append(x2)\n",
        "            L0.append(l0)\n",
        "            L1_up.append(l1_up)\n",
        "            L1_dn.append(l1_dn)\n",
        "            L2.append(l2)\n",
        "            label.append(l)\n",
        "\n",
        "            x0_total += x0_s\n",
        "            x1_total += x1_s\n",
        "            x2_total += x2_s\n",
        "            l0_total += l0_s\n",
        "            l1_u_total += l1_u_s\n",
        "            l1_d_total += l1_d_s\n",
        "            l2_total += l2_s\n",
        "            label_total += l_s\n",
        "\n",
        "            slices[\"X0\"].append(x0_total)\n",
        "            slices[\"X1\"].append(x1_total)\n",
        "            slices[\"X2\"].append(x2_total)\n",
        "            slices[\"L0\"].append(l0_total)\n",
        "            slices[\"L1_up\"].append(l1_u_total)\n",
        "            slices[\"L1_down\"].append(l1_d_total)\n",
        "            slices[\"L2\"].append(l2_total)\n",
        "            slices[\"label\"].append(label_total)\n",
        "\n",
        "            del data\n",
        "\n",
        "        del data_list\n",
        "\n",
        "        X0 = torch.cat(X0, dim=0).cpu()\n",
        "        X1 = torch.cat(X1, dim=0).cpu()\n",
        "        X2 = torch.cat(X2, dim=0).cpu()\n",
        "        L0 = torch.cat(L0, dim=-1).cpu()\n",
        "        L1_up = torch.cat(L1_up, dim=-1).cpu()\n",
        "        L1_down = torch.cat(L1_dn, dim=-1).cpu()\n",
        "        L2 = torch.cat(L2, dim=-1).cpu()\n",
        "        label = torch.cat(label, dim=-1).cpu()\n",
        "\n",
        "        data = SATComplex(X0, X1, X2, L0, L1_up, L1_down, L2, label)\n",
        "\n",
        "        return data, slices\n",
        "\n",
        "    def get(self, data, slices, idx):\n",
        "        x0_slice = slices[\"X0\"][idx:idx + 2]\n",
        "        x1_slice = slices[\"X1\"][idx:idx + 2]\n",
        "        x2_slice = slices[\"X2\"][idx:idx + 2]\n",
        "        l0_slice = slices[\"L0\"][idx:idx + 2]\n",
        "        l1_u_slice = slices[\"L1_up\"][idx:idx + 2]\n",
        "        l1_d_slice = slices[\"L1_down\"][idx:idx + 2]\n",
        "        l2_slice = slices[\"L2\"][idx:idx + 2]\n",
        "        label_slice = slices[\"label\"][idx: idx + 2]\n",
        "\n",
        "        X0 = data.X0[x0_slice[0]: x0_slice[1]]\n",
        "        X1 = data.X1[x1_slice[0]: x1_slice[1]]\n",
        "        X2 = data.X2[x2_slice[0]: x2_slice[1]]\n",
        "\n",
        "        L0 = data.L0[:, l0_slice[0]: l0_slice[1]]\n",
        "        L1_up = data.L1_up[:, l1_u_slice[0]: l1_u_slice[1]]\n",
        "        L1_dn = data.L1_down[:, l1_d_slice[0]: l1_d_slice[1]]\n",
        "        L2 = data.L2[:, l2_slice[0]: l2_slice[1]]\n",
        "\n",
        "        label = data.label[label_slice[0]: label_slice[1]]\n",
        "\n",
        "        return SATComplex(X0, X1, X2, L0, L1_up, L1_dn, L2, label)\n",
        "\n",
        "    def batch(self, objectList):\n",
        "        def unpack_SATComplex(SATComplex):\n",
        "            X0, X1, X2 = SATComplex.X0, SATComplex.X1, SATComplex.X2\n",
        "            L0, L1_u, L1_d, L2 = SATComplex.L0, SATComplex.L1_up, SATComplex.L1_down, SATComplex.L2\n",
        "\n",
        "            L0_i, L0_v = L0[0:2], L0[2:3].squeeze()\n",
        "            L1_u_i, L1_u_v = L1_u[0:2], L1_u[2:3].squeeze()\n",
        "            L1_d_i, L1_d_v = L1_d[0:2], L1_d[2:3].squeeze()\n",
        "            L2_i, L2_v = L2[0:2], L2[2:3].squeeze()\n",
        "\n",
        "            label = SATComplex.label\n",
        "            return [X0, X1, X1, X2], [L0_i, L1_u_i, L1_d_i, L2_i], [L0_v, L1_u_v, L1_d_v, L2_v], label\n",
        "\n",
        "        unpacked_grapObject = [unpack_SATComplex(g) for g in objectList]\n",
        "        X, L_i, L_v, labels = [*zip(*unpacked_grapObject)]\n",
        "        X, L_i, L_v = [*zip(*X)], [*zip(*L_i)], [*zip(*L_v)]\n",
        "\n",
        "        features_dct = batch_all_feature_and_lapacian_pair(X, L_i, L_v)\n",
        "\n",
        "        labels = torch.cat(labels, dim=0)\n",
        "\n",
        "        X0, X1, _, X2 = features_dct['features']\n",
        "        L0_i, L1_u_i, L1_d_i, L2_i = features_dct['lapacian_indices']\n",
        "        L0_v, L1_u_v, L1_d_v, L2_v = features_dct['lapacian_values']\n",
        "\n",
        "        L0 = torch.cat([L0_i, L0_v.unsqueeze(0)], dim=0)\n",
        "        L1_u = torch.cat([L1_u_i, L1_u_v.unsqueeze(0)], dim=0)\n",
        "        L1_d = torch.cat([L1_d_i, L1_d_v.unsqueeze(0)], dim=0)\n",
        "        L2 = torch.cat([L2_i, L2_v.unsqueeze(0)], dim=0)\n",
        "\n",
        "        batch = features_dct['batch_index']\n",
        "        del batch[1]\n",
        "\n",
        "        complex = SATComplex(X0, X1, X2, L0, L1_u, L1_d, L2, torch.tensor([0]), batch)\n",
        "        return complex, labels\n",
        "\n",
        "    def clean_features(self, satComplex):\n",
        "        satComplex.L0 = to_sparse_coo(satComplex.L0)\n",
        "        satComplex.L1_up = to_sparse_coo(satComplex.L1_up)\n",
        "        satComplex.L1_down = to_sparse_coo(satComplex.L1_down)\n",
        "        satComplex.L2 = to_sparse_coo(satComplex.L2)\n",
        "        return satComplex\n",
        "\n",
        "    def repair(self, satComplex):\n",
        "        return satComplex\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dlfgxUCZ2ldO"
      },
      "outputs": [],
      "source": [
        "#sat_model\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch_geometric.nn import global_mean_pool\n",
        "import torch.nn.functional as F\n",
        "import functools\n",
        "import warnings\n",
        "from typing import Optional, Tuple\n",
        "\n",
        "import torch\n",
        "from torch import Tensor\n",
        "from torch.nn.init import constant_, xavier_normal_, xavier_uniform_\n",
        "from torch.nn.parameter import Parameter\n",
        "\n",
        "\n",
        "class SATLayer_orientated(nn.Module):\n",
        "\n",
        "    def __init__(self, input_size, output_size, bias=True):\n",
        "        super().__init__()\n",
        "        self.a_1 = nn.Linear(output_size, 1, bias=bias)\n",
        "        self.a_2 = nn.Linear(output_size, 1, bias=bias)\n",
        "        self.a_3 = nn.Linear(output_size, 1, bias=bias)\n",
        "        self.layer = nn.Linear(input_size, output_size, bias=bias)\n",
        "\n",
        "    def forward(self, features, adj):\n",
        "        \"\"\"\n",
        "        features : n * m dense matrix of feature vectors\n",
        "        adj : n * n  sparse signed orientation matrix\n",
        "        output : n * k dense matrix of new feature vectors\n",
        "        \"\"\"\n",
        "        features = self.layer(features)\n",
        "        indices = adj.coalesce().indices()\n",
        "        values = adj.coalesce().values()\n",
        "\n",
        "        a_1 = self.a_1(features.abs())\n",
        "        a_2 = self.a_2(features.abs())\n",
        "        a_3 = self.a_3(features.abs())\n",
        "        v = (a_1 + a_2.T + a_3)[indices[0, :], indices[1, :]]\n",
        "        e = torch.sparse_coo_tensor(indices, v)\n",
        "        attention = torch.sparse.softmax(e, dim=1)\n",
        "        a_v = torch.mul(attention.coalesce().values(), values)\n",
        "        attention = torch.sparse_coo_tensor(indices, a_v)\n",
        "\n",
        "        output = torch.sparse.mm(attention, features)\n",
        "\n",
        "        return output\n",
        "\n",
        "\n",
        "class SATLayer_regular(nn.Module):\n",
        "\n",
        "    def __init__(self, input_size, output_size, bias=True):\n",
        "        super().__init__()\n",
        "        self.a_1 = nn.Linear(output_size, 1, bias=bias)\n",
        "        self.a_2 = nn.Linear(output_size, 1, bias=bias)\n",
        "        self.a_3 = nn.Linear(output_size, 1, bias=bias)        \n",
        "        self.layer = nn.Linear(input_size, output_size, bias=bias)\n",
        "\n",
        "    def forward(self, features, adj):\n",
        "        \"\"\"\n",
        "        features : n * m dense matrix of feature vectors\n",
        "        adj : n * n  sparse signed orientation matrix\n",
        "        output : n * k dense matrix of new feature vectors\n",
        "        \"\"\"\n",
        "        features = self.layer(features)\n",
        "        indices = adj.coalesce().indices()\n",
        "\n",
        "        a_1 = self.a_1(features)\n",
        "        a_2 = self.a_2(features)\n",
        "        a_3 = self.a_3(features)\n",
        "        v = (a_1 + a_2.T + a_3)[indices[0, :], indices[1, :]]\n",
        "        e = torch.sparse_coo_tensor(indices, v)\n",
        "        attention = torch.sparse.softmax(e, dim=1)\n",
        "\n",
        "        output = torch.sparse.mm(attention, features)\n",
        "\n",
        "        return output\n",
        "\n",
        "class PReLU(torch.nn.Module):\n",
        "    r\"\"\"Applies the element-wise function:\n",
        "\n",
        "    .. math::\n",
        "        \\text{PReLU}(x) = \\max(0,x) + a * \\min(0,x)\n",
        "\n",
        "    or\n",
        "\n",
        "    .. math::\n",
        "        \\text{PReLU}(x) =\n",
        "        \\begin{cases}\n",
        "        x, & \\text{ if } x \\geq 0 \\\\\n",
        "        ax, & \\text{ otherwise }\n",
        "        \\end{cases}\n",
        "\n",
        "    Here :math:`a` is a learnable parameter. When called without arguments, `nn.PReLU()` uses a single\n",
        "    parameter :math:`a` across all input channels. If called with `nn.PReLU(nChannels)`,\n",
        "    a separate :math:`a` is used for each input channel.\n",
        "\n",
        "\n",
        "    .. note::\n",
        "        weight decay should not be used when learning :math:`a` for good performance.\n",
        "\n",
        "    .. note::\n",
        "        Channel dim is the 2nd dim of input. When input has dims < 2, then there is\n",
        "        no channel dim and the number of channels = 1.\n",
        "\n",
        "    Args:\n",
        "        num_parameters (int): number of :math:`a` to learn.\n",
        "            Although it takes an int as input, there is only two values are legitimate:\n",
        "            1, or the number of channels at input. Default: 1\n",
        "        init (float): the initial value of :math:`a`. Default: 0.25\n",
        "\n",
        "    Shape:\n",
        "        - Input: :math:`( *)` where `*` means, any number of additional\n",
        "          dimensions.\n",
        "        - Output: :math:`(*)`, same shape as the input.\n",
        "\n",
        "    Attributes:\n",
        "        weight (Tensor): the learnable weights of shape (:attr:`num_parameters`).\n",
        "\n",
        "    .. image:: ../scripts/activation_images/PReLU.png\n",
        "\n",
        "    Examples::\n",
        "\n",
        "        >>> m = nn.PReLU()\n",
        "        >>> input = torch.randn(2)\n",
        "        >>> output = m(input)\n",
        "    \"\"\"\n",
        "    __constants__ = ['num_parameters']\n",
        "    num_parameters: int\n",
        "\n",
        "    def __init__(self, num_parameters: int = 1, const_val: float = 0.25,\n",
        "                 device=None, dtype=None) -> None:\n",
        "        factory_kwargs = {'device': device, 'dtype': dtype}\n",
        "        self.num_parameters = num_parameters\n",
        "        super(PReLU, self).__init__()\n",
        "        self.weight = Parameter(torch.empty(num_parameters, **factory_kwargs).fill_(const_val))\n",
        "    def forward(self, input: Tensor) -> Tensor:\n",
        "        return F.prelu(input, self.weight)\n",
        "\n",
        "    def extra_repr(self) -> str:\n",
        "        return 'num_parameters={}'.format(self.num_parameters)\n",
        "\n",
        "#class PRELU(nn.PReLU):\n",
        "\n",
        "    # def forward(self, input):\n",
        "    #     return nn.PReLU(input, self.weight)\n",
        "\n",
        "\n",
        "class PlanetoidSAT(nn.Module):\n",
        "\n",
        "    def __init__(self, num_node_feats, output_size, bias=True):\n",
        "        super().__init__()\n",
        "        k_heads = 2\n",
        "        self.layer_n = torch.nn.ModuleList([SATLayer_regular(num_node_feats, output_size, bias) for _ in range(k_heads)])\n",
        "        self.layer_e = torch.nn.ModuleList([SATLayer_regular(num_node_feats, output_size, bias) for _ in range(k_heads)])\n",
        "        self.layer_t = torch.nn.ModuleList([SATLayer_regular(num_node_feats, output_size, bias) for _ in range(k_heads)])\n",
        "        self.layer_s = torch.nn.ModuleList([SATLayer_regular(num_node_feats, output_size, bias) for _ in range(k_heads)])   \n",
        "        self.f = PReLU()\n",
        "\n",
        "        self.tri_layer = nn.Linear(output_size, output_size)\n",
        "\n",
        "    def forward(self, simplicialComplex, B1, B2):\n",
        "        X0, X1, X2 = simplicialComplex.unpack_features()\n",
        "        L0, _, L2 = simplicialComplex.unpack_laplacians()\n",
        "        L1 = simplicialComplex.unpack_up_down()\n",
        "\n",
        "        X0[X0 != 0] = 1\n",
        "\n",
        "        X1_in, X1_out = X0[X1[:, 0]], X0[X1[:, 1]]\n",
        "        X1 = torch.logical_and(X1_in, X1_out).float()\n",
        "\n",
        "        X2_i, X2_j, X2_k = X0[X2[:, 0]], X0[X2[:, 1]], X0[X2[:, 2]]\n",
        "        X2 = torch.logical_and(X2_i, torch.logical_and(X2_j, X2_k)).float()\n",
        "\n",
        "        X0 = self.f(functools.reduce(lambda a, b: a + b, [sat(X0, L0) for sat in self.layer_n]))\n",
        "        X1 = self.f(functools.reduce(lambda a, b: a + b, [sat(X1, L) for L, sat in zip(L1, self.layer_e)]))\n",
        "        X2 = self.f(functools.reduce(lambda a, b: a + b, [sat(X2, L2) for sat in self.layer_t]))\n",
        "\n",
        "        X0 = (X0 + torch.sparse.mm(B1, X1) + torch.sparse.mm(B1, self.tri_layer(torch.sparse.mm(B2, X2)))) / 3\n",
        "        return X0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TKo4DVYE9Yld"
      },
      "outputs": [],
      "source": [
        "#planetoid.logreg\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class LogReg(nn.Module):\n",
        "    def __init__(self, ft_in, nb_classes):\n",
        "        super(LogReg, self).__init__()\n",
        "        self.fc = nn.Linear(ft_in, nb_classes)\n",
        "\n",
        "        for m in self.modules():\n",
        "            self.weights_init(m)\n",
        "\n",
        "    def weights_init(self, m):\n",
        "        if isinstance(m, nn.Linear):\n",
        "            torch.nn.init.xavier_uniform_(m.weight.data)\n",
        "            if m.bias is not None:\n",
        "                m.bias.data.fill_(0.0)\n",
        "\n",
        "    def forward(self, seq):\n",
        "        ret = self.fc(seq)\n",
        "        return ret"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CGSfVS-L9YoK"
      },
      "outputs": [],
      "source": [
        "#planetoid.DGI\n",
        "#from models.nn_utils import convert_to_CoChain, torch_sparse_to_scipy_sparse, scipy_sparse_to_torch_sparse, \\\n",
        "#    normalise_boundary #+\n",
        "import scipy\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "#from constants import DEVICE #+\n",
        "\n",
        "\n",
        "def convert_to_device(lst):\n",
        "    return [i.to(DEVICE) for i in lst]\n",
        "\n",
        "\n",
        "def corruption_function(simplicialComplex, processor_type, p=0.000):\n",
        "    L0 = simplicialComplex.L0\n",
        "    X0 = simplicialComplex.X0\n",
        "    nb_nodes = X0.shape[0]\n",
        "    idx = np.random.permutation(nb_nodes)\n",
        "    # idx = [i for i in range(nb_nodes)]\n",
        "    C_X0 = X0[idx]\n",
        "\n",
        "    L0_i = L0.coalesce().indices().cpu()\n",
        "    L0_v = -torch.ones(L0_i.shape[1])\n",
        "    L0 = torch.sparse_coo_tensor(L0_i, L0_v)\n",
        "    cor_adj_i = torch.triu_indices(nb_nodes, nb_nodes, 0)\n",
        "    cor_adj_v = torch.tensor(np.random.binomial(1, p, size=(cor_adj_i.shape[1])), dtype=torch.float)\n",
        "\n",
        "    # logical xor for edge insertion/deletion\n",
        "    cor_adj = torch.sparse_coo_tensor(cor_adj_i, cor_adj_v)\n",
        "    cor_adj = L0 + cor_adj\n",
        "    cor_adj_i, cor_adj_v = cor_adj.coalesce().indices(), cor_adj.coalesce().values()\n",
        "    cor_adj_v = torch.abs(cor_adj_v)\n",
        "    cor_adj = torch.sparse_coo_tensor(cor_adj_i, cor_adj_v)\n",
        "    cor_adj = torch_sparse_to_scipy_sparse(cor_adj)\n",
        "    cor_adj = scipy.sparse.triu(cor_adj, k=1)\n",
        "    cor_adj.eliminate_zeros()\n",
        "    cor_adj = scipy_sparse_to_torch_sparse(cor_adj)\n",
        "\n",
        "    fake_labels = torch.zeros(nb_nodes)\n",
        "    cochain = convert_to_CoChain(cor_adj, C_X0, fake_labels)\n",
        "    corrupted_train = processor_type.process(cochain)\n",
        "    corrupted_train = processor_type.batch([corrupted_train])[0]\n",
        "    corrupted_train = processor_type.clean_features(corrupted_train)\n",
        "    corrupted_train = processor_type.repair(corrupted_train)\n",
        "\n",
        "    b1, b2 = normalise_boundary(cochain.b1, cochain.b2)\n",
        "\n",
        "    return corrupted_train, b1, b2\n",
        "\n",
        "\n",
        "######################################################################################################\n",
        "# This section is adopted from https://github.com/PetarV-/DGI/tree/61baf67d7052905c77bdeb28c22926f04e182362\n",
        "######################################################################################################\n",
        "\n",
        "class AvgReadout(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(AvgReadout, self).__init__()\n",
        "\n",
        "    def forward(self, seq, msk):\n",
        "        if msk is None:\n",
        "            return torch.mean(seq, 1)\n",
        "        else:\n",
        "            msk = torch.unsqueeze(msk, -1)\n",
        "            return torch.sum(seq * msk, 1) / torch.sum(msk)\n",
        "\n",
        "\n",
        "class Discriminator(nn.Module):\n",
        "    def __init__(self, n_h):\n",
        "        super(Discriminator, self).__init__()\n",
        "        self.f_k = nn.Bilinear(n_h, n_h, 1)\n",
        "\n",
        "        for m in self.modules():\n",
        "            self.weights_init(m.to(DEVICE))\n",
        "\n",
        "    def weights_init(self, m):\n",
        "        if isinstance(m, nn.Bilinear):\n",
        "            torch.nn.init.xavier_uniform_(m.weight.data)\n",
        "            if m.bias is not None:\n",
        "                m.bias.data.fill_(0.0)\n",
        "\n",
        "    def forward(self, c, h_pl, h_mi):\n",
        "        c_x = torch.unsqueeze(c, 1)\n",
        "        c_x = c_x.expand_as(h_pl)\n",
        "\n",
        "        sc_1 = torch.squeeze(self.f_k(h_pl, c_x), 2)\n",
        "        sc_2 = torch.squeeze(self.f_k(h_mi, c_x), 2)\n",
        "\n",
        "        logits = torch.cat((sc_1, sc_2), 1)\n",
        "\n",
        "        return logits\n",
        "\n",
        "\n",
        "class DGI(nn.Module):\n",
        "    def __init__(self, input_size, output_size, model):\n",
        "        super(DGI, self).__init__()\n",
        "        self.model = model(input_size, output_size).to(DEVICE)\n",
        "        self.read = AvgReadout()\n",
        "\n",
        "        self.sigm = nn.SELU()\n",
        "\n",
        "        self.disc = Discriminator(output_size)\n",
        "\n",
        "    def forward(self, simplicialComplex, b1, b2, processor_type):\n",
        "        corrupted_complex, cb1, cb2 = corruption_function(simplicialComplex, processor_type)\n",
        "        simplicialComplex.to_device()\n",
        "        corrupted_complex.to_device()\n",
        "        cb1 = cb1.to(DEVICE)\n",
        "        cb2 = cb2.to(DEVICE)\n",
        "\n",
        "        h_1 = self.model(simplicialComplex, b1, b2).unsqueeze(0)\n",
        "        c = self.read(h_1, None)\n",
        "        c = self.sigm(c)\n",
        "\n",
        "        h_2 = self.model(corrupted_complex, cb1, cb2).unsqueeze(0)\n",
        "\n",
        "        ret = self.disc(c, h_1, h_2)\n",
        "\n",
        "        return ret\n",
        "\n",
        "    # Detach the return variables\n",
        "    def embed(self, simplicialComplex, b1, b2):\n",
        "        simplicialComplex.to_device()\n",
        "        h_1 = self.model(simplicialComplex, b1, b2)\n",
        "        c = self.read(h_1, None)\n",
        "\n",
        "        return h_1.detach(), c.detach()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bhN9dJM479Aw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4bed0d98-38f7-4e11-e7fc-479ec506364e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.x\n",
            "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.tx\n",
            "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.allx\n",
            "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.y\n",
            "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.ty\n",
            "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.ally\n",
            "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.graph\n",
            "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.test.index\n",
            "Processing...\n",
            "Done!\n"
          ]
        }
      ],
      "source": [
        "#fake_dataset\n",
        "from torch_geometric.datasets import Planetoid\n",
        "import torch\n",
        "import networkx as nx\n",
        "#from models.nn_utils import convert_to_CoChain, to_sparse_coo #+\n",
        "#from utils import edge_to_node_matrix, triangle_to_edge_matrix #+\n",
        "\n",
        "\n",
        "class GraphObject:\n",
        "\n",
        "    def __init__(self, x, edge_index, y, train_mask, val_mask, test_mask):\n",
        "        self.x = x\n",
        "        self.edge_index = edge_index\n",
        "        self.y = y\n",
        "        self.train_mask = train_mask\n",
        "        self.val_mask = val_mask\n",
        "        self.test_mask = test_mask\n",
        "\n",
        "def gen_dataset():\n",
        "    data = Planetoid('./data', \"Cora\")[0]\n",
        "\n",
        "    edges = data.edge_index\n",
        "    n = data.x.shape[0]\n",
        "    adj = torch.zeros((n, n))\n",
        "    adj = adj.index_put_(tuple(edges), torch.ones(1))\n",
        "    adj = torch.triu(adj)\n",
        "    edges = torch.nonzero(adj).tolist()\n",
        "    nodes = [i for i in range(n)]\n",
        "\n",
        "    g = nx.Graph()\n",
        "    g.add_nodes_from(nodes)\n",
        "    g.add_edges_from(edges)\n",
        "\n",
        "    triangles = [list(sorted(x)) for x in nx.enumerate_all_cliques(g) if len(x) == 3]\n",
        "    quads = [list(sorted(x)) for x in nx.enumerate_all_cliques(g) if len(x) == 4]\n",
        "    quads_indices_set = set()\n",
        "\n",
        "    labels = [0 for _ in range(n)]\n",
        "    for quad in quads:\n",
        "        for index in quad:\n",
        "            labels[index] = 2\n",
        "            quads_indices_set.add(index)\n",
        "\n",
        "    tri_indices_set = set()\n",
        "\n",
        "    for triangle in triangles:\n",
        "        for index in triangle:\n",
        "            if index not in quads_indices_set:\n",
        "                labels[index] = 1\n",
        "                tri_indices_set.add(index)\n",
        "\n",
        "    y = torch.tensor(labels)\n",
        "\n",
        "    train_mask = []\n",
        "    val_mask = []\n",
        "    test_mask = []\n",
        "\n",
        "    train_no = 60\n",
        "    val_no = 300\n",
        "    test_no = 1000\n",
        "\n",
        "    class_1 = 0\n",
        "    class_2 = 0\n",
        "    class_3 = 0\n",
        "\n",
        "    val = 0\n",
        "    test = 0\n",
        "\n",
        "    for i in nodes:\n",
        "        if class_1 + class_2 + class_3 < train_no:\n",
        "            if i in tri_indices_set and class_1 < 20:\n",
        "                train_mask.append(i)\n",
        "                class_1 += 1\n",
        "            elif i in quads_indices_set and class_2 < 20:\n",
        "                train_mask.append(i)\n",
        "                class_2 += 1\n",
        "            elif class_3 < 20:\n",
        "                train_mask.append(i)\n",
        "                class_3 += 1\n",
        "        elif val < val_no:\n",
        "            val_mask.append(i)\n",
        "            val += 1\n",
        "        elif test < test_no:\n",
        "            test_mask.append(i)\n",
        "            test += 1\n",
        "\n",
        "    train_index = torch.tensor(train_mask)\n",
        "    train_mask = torch.zeros(n)\n",
        "    train_mask.index_fill_(0, train_index, 1)\n",
        "    train_mask = train_mask > 0\n",
        "\n",
        "    test_index = torch.tensor(test_mask)\n",
        "    test_mask = torch.zeros(n)\n",
        "    test_mask.index_fill_(0, test_index, 1)\n",
        "    test_mask = test_mask > 0\n",
        "\n",
        "    val_index = torch.tensor(val_mask)\n",
        "    val_mask = torch.zeros(n)\n",
        "    val_mask.index_fill_(0, val_index, 1)\n",
        "    val_mask = val_mask > 0\n",
        "\n",
        "    # X0 = torch.sum(adj, dim = 1)\n",
        "    # X0 = torch.nn.functional.one_hot(X0.long()).float()\n",
        "    # X0 = adj + adj.T\n",
        "    X0 = torch.ones((adj.shape[0], adj.shape[0]))\n",
        "    edge_index = torch.nonzero(adj).T\n",
        "\n",
        "    assert (X0.shape[0] == y.shape[0])\n",
        "    assert (y.shape[0] == train_mask.shape[0])\n",
        "    assert (y.shape[0] == val_mask.shape[0])\n",
        "    assert (y.shape[0] == test_mask.shape[0])\n",
        "\n",
        "    g = GraphObject(X0, edge_index, y, train_mask, val_mask, test_mask)\n",
        "    return g\n",
        "\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    gen_dataset()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3iHLPEah79D5"
      },
      "outputs": [],
      "source": [
        "#planetoid_dataset\n",
        "from torch_geometric.datasets import Planetoid\n",
        "from torch_geometric.data import InMemoryDataset\n",
        "import numpy as np\n",
        "import torch\n",
        "#from models.nn_utils import convert_to_CoChain, remove_diag_sparse, to_sparse_coo, normalise_boundary, preprocess_features #+\n",
        "#from Planetoid.FakeDataset import gen_dataset #+\n",
        "\n",
        "\n",
        "class PlanetoidSCDataset(InMemoryDataset):\n",
        "\n",
        "    def __init__(self, root, dataset_name, processor_type):\n",
        "        self.root = root\n",
        "        self.dataset_name = dataset_name\n",
        "        self.processor_type = processor_type\n",
        "\n",
        "        folder = f\"{root}/{self.dataset_name}/{processor_type.__class__.__name__}\"\n",
        "\n",
        "        super().__init__(folder)\n",
        "        self.data, self.slices = torch.load(self.processed_paths[0])\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.slices[\"X0\"]) - 1\n",
        "\n",
        "    def load_dataset(self):\n",
        "        \"\"\"Load the dataset_processor from here and process it if it doesn't exist\"\"\"\n",
        "        print(\"Loading dataset_processor from disk...\")\n",
        "        data, slices = torch.load(self.processed_paths[0])\n",
        "        return data, slices\n",
        "\n",
        "    @property\n",
        "    def raw_file_names(self):\n",
        "        return []\n",
        "\n",
        "    def download(self):\n",
        "        # Instantiating this will download and process the graph dataset_processor.\n",
        "        if self.dataset_name == 'fake':\n",
        "            self.data_download = gen_dataset()\n",
        "        else:\n",
        "            self.data_download = Planetoid(self.root, self.dataset_name)[0]\n",
        "        nodes = self.data_download.x.shape[0]\n",
        "        self.nodes = np.array([i for i in range(nodes)])\n",
        "\n",
        "        self.test_split = self.data_download.test_mask\n",
        "        self.train_split = self.data_download.train_mask\n",
        "        self.val_split = self.data_download.val_mask\n",
        "\n",
        "    @property\n",
        "    def processed_file_names(self):\n",
        "        return [\"features.pt\"]\n",
        "\n",
        "    def process(self):\n",
        "        data = self.data_download\n",
        "        features, edges, labels = data.x, data.edge_index, data.y\n",
        "        adj_ones = torch.ones(edges.shape[1])\n",
        "        adj = torch.sparse_coo_tensor(edges, adj_ones)\n",
        "        # features = preprocess_features(features)\n",
        "        adj = remove_diag_sparse(adj)\n",
        "        dataset = convert_to_CoChain(adj, features, labels)\n",
        "        dataset = [self.processor_type.process(dataset)]\n",
        "        data, slices = self.processor_type.collate(dataset)\n",
        "        torch.save((data, slices), self.processed_paths[0])\n",
        "\n",
        "    def get_boundary(self, edge_list, features):\n",
        "        adj_ones = torch.ones(edge_list.shape[1])\n",
        "        adj = torch.sparse_coo_tensor(edge_list, adj_ones)\n",
        "\n",
        "        # features = preprocess_features(features)\n",
        "        adj = remove_diag_sparse(adj)\n",
        "        cochain = convert_to_CoChain(adj, features, None)\n",
        "        b1, b2 = normalise_boundary(cochain.b1, cochain.b2)\n",
        "        return b1, b2\n",
        "\n",
        "    def _get_node_subsection(self, idx_list):\n",
        "        dataset = self.__getitem__(0)\n",
        "        idx_list = torch.tensor(idx_list)\n",
        "        adj = to_sparse_coo(dataset.L0).to_dense()\n",
        "        adj = torch.index_select(adj, 0, idx_list)\n",
        "        adj = torch.index_select(adj, 1, idx_list)\n",
        "        adj = torch.triu(adj, diagonal=1).to_sparse()\n",
        "        features = dataset.X0[idx_list]\n",
        "        labels = dataset.label[idx_list]\n",
        "        simplicialComplex = convert_to_CoChain(adj, features, labels)\n",
        "        simplicialComplex = self.processor_type.process(simplicialComplex)\n",
        "        simplicialComplex = self.processor_type.batch([simplicialComplex])[0]\n",
        "        simplicialComplex = self.processor_type.clean_features(simplicialComplex)\n",
        "        return simplicialComplex\n",
        "\n",
        "    def get_full(self):\n",
        "        simplicialComplex = self.get(0)\n",
        "        simplicialComplex = self.processor_type.batch([simplicialComplex])[0]\n",
        "        simplicialComplex = self.processor_type.clean_features(simplicialComplex)\n",
        "        simplicialComplex = self.processor_type.repair(simplicialComplex)\n",
        "        b1, b2 = self.get_boundary(simplicialComplex.L0.coalesce().indices(), simplicialComplex.X0)\n",
        "        return simplicialComplex, b1, b2\n",
        "\n",
        "    def get_train_labels(self):\n",
        "        simplicialComplex = self.get(0)\n",
        "        return simplicialComplex.label[self.train_split]\n",
        "\n",
        "    def get_val_labels(self):\n",
        "        simplicialComplex = self.get(0)\n",
        "        return simplicialComplex.label[self.val_split]\n",
        "\n",
        "    def get_test_labels(self):\n",
        "        simplicialComplex = self.get(0)\n",
        "        return simplicialComplex.label[self.test_split]\n",
        "\n",
        "    def get_labels(self):\n",
        "        simplicialComplex = self.get(0)\n",
        "        return simplicialComplex.label\n",
        "\n",
        "    def get_train_embeds(self, embeds):\n",
        "        return embeds[self.train_split]\n",
        "\n",
        "    def get_val_embeds(self, embeds):\n",
        "        return embeds[self.val_split]\n",
        "\n",
        "    def get_test_embeds(self, embeds):\n",
        "        return embeds[self.test_split]\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.processor_type.get(self.data, self.slices, idx)\n",
        "\n",
        "    def get_name(self):\n",
        "        return self.name\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7779--K4_c53"
      },
      "outputs": [],
      "source": [
        "planetoid_SAT = [SATProcessor(), PlanetoidSAT]\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tb-nightly\n",
        "!pip install future"
      ],
      "metadata": {
        "id": "vpn3zLvn2Z9g",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b1293557-527c-4608-cf70-30907fd78d2b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting tb-nightly\n",
            "  Downloading tb_nightly-2.13.0a20230326-py3-none-any.whl (5.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.6/5.6 MB\u001b[0m \u001b[31m41.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.9/dist-packages (from tb-nightly) (2.27.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.9/dist-packages (from tb-nightly) (3.4.2)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.9/dist-packages (from tb-nightly) (1.4.0)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.9/dist-packages (from tb-nightly) (0.40.0)\n",
            "Collecting tensorboard-data-server<0.8.0,>=0.7.0\n",
            "  Downloading tensorboard_data_server-0.7.0-py3-none-manylinux2014_x86_64.whl (6.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.6/6.6 MB\u001b[0m \u001b[31m56.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.12.0 in /usr/local/lib/python3.9/dist-packages (from tb-nightly) (1.22.4)\n",
            "Requirement already satisfied: protobuf>=3.19.6 in /usr/local/lib/python3.9/dist-packages (from tb-nightly) (3.19.6)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.9/dist-packages (from tb-nightly) (67.6.0)\n",
            "Collecting google-auth-oauthlib<1.1,>=0.5\n",
            "  Downloading google_auth_oauthlib-1.0.0-py2.py3-none-any.whl (18 kB)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.9/dist-packages (from tb-nightly) (2.16.2)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.9/dist-packages (from tb-nightly) (1.8.1)\n",
            "Requirement already satisfied: grpcio>=1.48.2 in /usr/local/lib/python3.9/dist-packages (from tb-nightly) (1.51.3)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.9/dist-packages (from tb-nightly) (2.2.3)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.9/dist-packages (from google-auth<3,>=1.6.3->tb-nightly) (4.9)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.9/dist-packages (from google-auth<3,>=1.6.3->tb-nightly) (5.3.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.9/dist-packages (from google-auth<3,>=1.6.3->tb-nightly) (0.2.8)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.9/dist-packages (from google-auth<3,>=1.6.3->tb-nightly) (1.16.0)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.9/dist-packages (from google-auth-oauthlib<1.1,>=0.5->tb-nightly) (1.3.1)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.9/dist-packages (from markdown>=2.6.8->tb-nightly) (6.1.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests<3,>=2.21.0->tb-nightly) (3.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests<3,>=2.21.0->tb-nightly) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests<3,>=2.21.0->tb-nightly) (2.0.12)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests<3,>=2.21.0->tb-nightly) (1.26.15)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.9/dist-packages (from werkzeug>=1.0.1->tb-nightly) (2.1.2)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.9/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tb-nightly) (3.15.0)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.9/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tb-nightly) (0.4.8)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.9/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tb-nightly) (3.2.2)\n",
            "Installing collected packages: tensorboard-data-server, google-auth-oauthlib, tb-nightly\n",
            "  Attempting uninstall: tensorboard-data-server\n",
            "    Found existing installation: tensorboard-data-server 0.6.1\n",
            "    Uninstalling tensorboard-data-server-0.6.1:\n",
            "      Successfully uninstalled tensorboard-data-server-0.6.1\n",
            "  Attempting uninstall: google-auth-oauthlib\n",
            "    Found existing installation: google-auth-oauthlib 0.4.6\n",
            "    Uninstalling google-auth-oauthlib-0.4.6:\n",
            "      Successfully uninstalled google-auth-oauthlib-0.4.6\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tensorboard 2.11.2 requires google-auth-oauthlib<0.5,>=0.4.1, but you have google-auth-oauthlib 1.0.0 which is incompatible.\n",
            "tensorboard 2.11.2 requires tensorboard-data-server<0.7.0,>=0.6.0, but you have tensorboard-data-server 0.7.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed google-auth-oauthlib-1.0.0 tb-nightly-2.13.0a20230326 tensorboard-data-server-0.7.0\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.9/dist-packages (0.18.3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "iua9znew2wyr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "2708, 79\n",
        "dataset = 'Cora'\n",
        "dataset_features_dct = {'Cora' : 1433, 'CiteSeer' : 3703, 'PubMed' : 500, 'fake' : 2708}\n",
        "dataset_classes_dct = {'Cora' : 7, 'CiteSeer' : 6, 'PubMed' : 3 , 'fake' : 3}\n",
        "input_size = dataset_features_dct[dataset]\n",
        "output_size = 512\n",
        "nb_epochs = 50\n",
        "test_epochs = 5\n",
        "lr = 0.001\n",
        "l2_coef = 0.0\n",
        "patience = 20\n",
        "\n",
        "# nn_mod = planetoid_GCN\n",
        "# nn_mod = planetoid_GAT\n",
        "# nn_mod = planetoid_SCN\n",
        "# nn_mod = planetoid_SCConv\n",
        "nn_mod = planetoid_SAT\n",
        "# nn_mod = planetoid_SAN\n",
        "\n",
        "processor_type = nn_mod[0]\n",
        "model = nn_mod[1]\n",
        "\n",
        "dgi = DGI(input_size, output_size, model)\n",
        "print(dgi)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ji6D986_wybf",
        "outputId": "6de3162c-97a9-43bf-9455-560169e34e47"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DGI(\n",
            "  (model): PlanetoidSAT(\n",
            "    (layer_n): ModuleList(\n",
            "      (0): SATLayer_regular(\n",
            "        (a_1): Linear(in_features=512, out_features=1, bias=True)\n",
            "        (a_2): Linear(in_features=512, out_features=1, bias=True)\n",
            "        (a_3): Linear(in_features=512, out_features=1, bias=True)\n",
            "        (layer): Linear(in_features=1433, out_features=512, bias=True)\n",
            "      )\n",
            "      (1): SATLayer_regular(\n",
            "        (a_1): Linear(in_features=512, out_features=1, bias=True)\n",
            "        (a_2): Linear(in_features=512, out_features=1, bias=True)\n",
            "        (a_3): Linear(in_features=512, out_features=1, bias=True)\n",
            "        (layer): Linear(in_features=1433, out_features=512, bias=True)\n",
            "      )\n",
            "    )\n",
            "    (layer_e): ModuleList(\n",
            "      (0): SATLayer_regular(\n",
            "        (a_1): Linear(in_features=512, out_features=1, bias=True)\n",
            "        (a_2): Linear(in_features=512, out_features=1, bias=True)\n",
            "        (a_3): Linear(in_features=512, out_features=1, bias=True)\n",
            "        (layer): Linear(in_features=1433, out_features=512, bias=True)\n",
            "      )\n",
            "      (1): SATLayer_regular(\n",
            "        (a_1): Linear(in_features=512, out_features=1, bias=True)\n",
            "        (a_2): Linear(in_features=512, out_features=1, bias=True)\n",
            "        (a_3): Linear(in_features=512, out_features=1, bias=True)\n",
            "        (layer): Linear(in_features=1433, out_features=512, bias=True)\n",
            "      )\n",
            "    )\n",
            "    (layer_t): ModuleList(\n",
            "      (0): SATLayer_regular(\n",
            "        (a_1): Linear(in_features=512, out_features=1, bias=True)\n",
            "        (a_2): Linear(in_features=512, out_features=1, bias=True)\n",
            "        (a_3): Linear(in_features=512, out_features=1, bias=True)\n",
            "        (layer): Linear(in_features=1433, out_features=512, bias=True)\n",
            "      )\n",
            "      (1): SATLayer_regular(\n",
            "        (a_1): Linear(in_features=512, out_features=1, bias=True)\n",
            "        (a_2): Linear(in_features=512, out_features=1, bias=True)\n",
            "        (a_3): Linear(in_features=512, out_features=1, bias=True)\n",
            "        (layer): Linear(in_features=1433, out_features=512, bias=True)\n",
            "      )\n",
            "    )\n",
            "    (layer_s): ModuleList(\n",
            "      (0): SATLayer_regular(\n",
            "        (a_1): Linear(in_features=512, out_features=1, bias=True)\n",
            "        (a_2): Linear(in_features=512, out_features=1, bias=True)\n",
            "        (a_3): Linear(in_features=512, out_features=1, bias=True)\n",
            "        (layer): Linear(in_features=1433, out_features=512, bias=True)\n",
            "      )\n",
            "      (1): SATLayer_regular(\n",
            "        (a_1): Linear(in_features=512, out_features=1, bias=True)\n",
            "        (a_2): Linear(in_features=512, out_features=1, bias=True)\n",
            "        (a_3): Linear(in_features=512, out_features=1, bias=True)\n",
            "        (layer): Linear(in_features=1433, out_features=512, bias=True)\n",
            "      )\n",
            "    )\n",
            "    (f): PReLU(num_parameters=1)\n",
            "    (tri_layer): Linear(in_features=512, out_features=512, bias=True)\n",
            "  )\n",
            "  (read): AvgReadout()\n",
            "  (sigm): SELU()\n",
            "  (disc): Discriminator(\n",
            "    (f_k): Bilinear(in1_features=512, in2_features=512, out_features=1, bias=True)\n",
            "  )\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nRyxZGuD79JK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b05a2915-3c15-4e4f-8875-9443417bc138"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.x\n",
            "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.tx\n",
            "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.allx\n",
            "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.y\n",
            "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.ty\n",
            "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.ally\n",
            "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.graph\n",
            "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.test.index\n",
            "Processing...\n",
            "Done!\n",
            "Processing...\n",
            "Done!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss: tensor(0.6931, device='cuda:0',\n",
            "       grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
            "Loss: tensor(0.6929, device='cuda:0',\n",
            "       grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
            "Loss: tensor(0.6920, device='cuda:0',\n",
            "       grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
            "Loss: tensor(0.6901, device='cuda:0',\n",
            "       grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
            "Loss: tensor(0.6868, device='cuda:0',\n",
            "       grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
            "Loss: tensor(0.6805, device='cuda:0',\n",
            "       grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
            "Loss: tensor(0.6724, device='cuda:0',\n",
            "       grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
            "Loss: tensor(0.6580, device='cuda:0',\n",
            "       grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
            "Loss: tensor(0.6437, device='cuda:0',\n",
            "       grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
            "Loss: tensor(0.6246, device='cuda:0',\n",
            "       grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
            "Loss: tensor(0.6171, device='cuda:0',\n",
            "       grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
            "Loss: tensor(0.5989, device='cuda:0',\n",
            "       grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
            "Loss: tensor(0.5983, device='cuda:0',\n",
            "       grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
            "Loss: tensor(0.5882, device='cuda:0',\n",
            "       grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
            "Loss: tensor(0.5775, device='cuda:0',\n",
            "       grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
            "Loss: tensor(0.5580, device='cuda:0',\n",
            "       grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
            "Loss: tensor(0.5367, device='cuda:0',\n",
            "       grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
            "Loss: tensor(0.5262, device='cuda:0',\n",
            "       grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
            "Loss: tensor(0.5132, device='cuda:0',\n",
            "       grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
            "Loss: tensor(0.4966, device='cuda:0',\n",
            "       grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
            "Loss: tensor(0.4775, device='cuda:0',\n",
            "       grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
            "Loss: tensor(0.4662, device='cuda:0',\n",
            "       grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
            "Loss: tensor(0.4690, device='cuda:0',\n",
            "       grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
            "Loss: tensor(0.4669, device='cuda:0',\n",
            "       grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
            "Loss: tensor(0.4391, device='cuda:0',\n",
            "       grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
            "Loss: tensor(0.4171, device='cuda:0',\n",
            "       grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
            "Loss: tensor(0.4052, device='cuda:0',\n",
            "       grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
            "Loss: tensor(0.3953, device='cuda:0',\n",
            "       grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
            "Loss: tensor(0.3817, device='cuda:0',\n",
            "       grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
            "Loss: tensor(0.3559, device='cuda:0',\n",
            "       grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
            "Loss: tensor(0.3727, device='cuda:0',\n",
            "       grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
            "Loss: tensor(0.3338, device='cuda:0',\n",
            "       grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
            "Loss: tensor(0.3193, device='cuda:0',\n",
            "       grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
            "Loss: tensor(0.3143, device='cuda:0',\n",
            "       grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
            "Loss: tensor(0.2943, device='cuda:0',\n",
            "       grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
            "Loss: tensor(0.2982, device='cuda:0',\n",
            "       grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
            "Loss: tensor(0.2873, device='cuda:0',\n",
            "       grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
            "Loss: tensor(0.2579, device='cuda:0',\n",
            "       grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
            "Loss: tensor(0.2604, device='cuda:0',\n",
            "       grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
            "Loss: tensor(0.2490, device='cuda:0',\n",
            "       grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
            "Loss: tensor(0.2348, device='cuda:0',\n",
            "       grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
            "Loss: tensor(0.2248, device='cuda:0',\n",
            "       grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
            "Loss: tensor(0.2220, device='cuda:0',\n",
            "       grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
            "Loss: tensor(0.2164, device='cuda:0',\n",
            "       grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
            "Loss: tensor(0.2022, device='cuda:0',\n",
            "       grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
            "Loss: tensor(0.1926, device='cuda:0',\n",
            "       grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
            "Loss: tensor(0.1911, device='cuda:0',\n",
            "       grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
            "Loss: tensor(0.1856, device='cuda:0',\n",
            "       grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
            "Loss: tensor(0.1749, device='cuda:0',\n",
            "       grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
            "Loss: tensor(0.1817, device='cuda:0',\n",
            "       grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
            "Loading 48th epoch\n",
            "PlanetoidSAT\n",
            "tensor(0.8030, device='cuda:0')\n",
            "PlanetoidSAT\n",
            "tensor(0.8050, device='cuda:0')\n",
            "PlanetoidSAT\n",
            "tensor(0.8020, device='cuda:0')\n",
            "PlanetoidSAT\n",
            "tensor(0.8080, device='cuda:0')\n",
            "PlanetoidSAT\n",
            "tensor(0.8120, device='cuda:0')\n",
            "Average accuracy: tensor([0.8060], device='cuda:0')\n",
            "tensor(80.6000, device='cuda:0')\n",
            "tensor(0.4062, device='cuda:0')\n"
          ]
        }
      ],
      "source": [
        "#from Planetoid.PlanetoidDataset import PlanetoidSCDataset #+\n",
        "#from models import planetoid_GCN, planetoid_GAT, planetoid_SCN, planetoid_SCConv, planetoid_SAN, planetoid_SAT\n",
        "import torch.nn as nn\n",
        "import torch\n",
        "#from Planetoid.DGI import DGI #+\n",
        "#from Planetoid.logreg import LogReg #+\n",
        "#from constants import DEVICE #+\n",
        "\n",
        "2708, 79\n",
        "dataset = 'Cora'\n",
        "dataset_features_dct = {'Cora' : 1433, 'CiteSeer' : 3703, 'PubMed' : 500, 'fake' : 2708}\n",
        "dataset_classes_dct = {'Cora' : 7, 'CiteSeer' : 6, 'PubMed' : 3 , 'fake' : 3}\n",
        "input_size = dataset_features_dct[dataset]\n",
        "output_size = 512\n",
        "nb_epochs = 50\n",
        "test_epochs = 5\n",
        "lr = 0.001\n",
        "l2_coef = 0.0\n",
        "patience = 20\n",
        "\n",
        "# nn_mod = planetoid_GCN\n",
        "# nn_mod = planetoid_GAT\n",
        "# nn_mod = planetoid_SCN\n",
        "# nn_mod = planetoid_SCConv\n",
        "nn_mod = planetoid_SAT\n",
        "# nn_mod = planetoid_SAN\n",
        "\n",
        "processor_type = nn_mod[0]\n",
        "model = nn_mod[1]\n",
        "\n",
        "dgi = DGI(input_size, output_size, model)\n",
        "optimiser = torch.optim.Adam(dgi.parameters(), lr=lr, weight_decay=l2_coef)#, momentum = 0.9, nesterov = True)\n",
        "b_xent = nn.BCEWithLogitsLoss()\n",
        "xent = nn.CrossEntropyLoss()\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "\n",
        "    data = PlanetoidSCDataset('./data', dataset, processor_type)\n",
        "     \n",
        "    for v in data.slices.items():\n",
        "      data.slices = [v]\n",
        "    \n",
        "    data_full, b1, b2 = data.get_full()\n",
        "\n",
        "    cnt_wait = 0\n",
        "    best = 1e9\n",
        "    best_t = 0\n",
        "    bl = False\n",
        "    b1 = b1.to(DEVICE)\n",
        "    b2 = b2.to(DEVICE)\n",
        "    for epoch in range(nb_epochs):\n",
        "        dgi.train()\n",
        "        optimiser.zero_grad()\n",
        "\n",
        "        nb_nodes = data_full.X0.shape[0]\n",
        "        lbl_1 = torch.ones(1, nb_nodes)\n",
        "        lbl_2 = torch.zeros(1, nb_nodes)\n",
        "\n",
        "        lbl = torch.cat((lbl_1, lbl_2), 1).to(DEVICE)\n",
        "\n",
        "        logits = dgi(data_full, b1, b2, processor_type)\n",
        "\n",
        "        loss = b_xent(logits, lbl)\n",
        "\n",
        "        print('Loss:', loss)\n",
        "\n",
        "        if loss < best:\n",
        "            best = loss\n",
        "            best_t = epoch\n",
        "            cnt_wait = 0\n",
        "            torch.save(dgi.state_dict(), f'./data/{model.__name__}_dgi.pkl')\n",
        "            if epoch != 0:\n",
        "                bl = True\n",
        "        else:\n",
        "            if bl:\n",
        "                cnt_wait += 1\n",
        "\n",
        "        if cnt_wait == patience:\n",
        "            print('Early stopping!')\n",
        "            break\n",
        "\n",
        "        loss.backward()\n",
        "        optimiser.step()\n",
        "\n",
        "    print('Loading {}th epoch'.format(best_t))\n",
        "    dgi.load_state_dict(torch.load(f'./data/{model.__name__}_dgi.pkl'))\n",
        "\n",
        "    embeds, _ = dgi.embed(data_full, b1, b2)\n",
        "    # embeds = data_full.X0.to(DEVICE)\n",
        "    # output_size = 79\n",
        "    # with open(\"./embeddings.py\", 'w') as f:\n",
        "    #     f.write(f'embeddings = {embeds.tolist()}')\n",
        "    # with open(\"./labels.py\", 'w') as f:\n",
        "    #     f.write(f'labels = {data.get_labels().tolist()}')\n",
        "    train_embs = data.get_train_embeds(embeds)\n",
        "    val_embs = data.get_val_embeds(embeds)\n",
        "    test_embs = data.get_test_embeds(embeds)\n",
        "\n",
        "    train_lbls = data.get_train_labels().to(DEVICE)\n",
        "    x_unique = train_lbls.unique(sorted=True)\n",
        "    x_unique_count = torch.stack([(train_lbls == x_u).sum() for x_u in x_unique])\n",
        "    val_lbls = data.get_val_labels().to(DEVICE)\n",
        "    test_lbls = data.get_test_labels().to(DEVICE)\n",
        "\n",
        "    tot = torch.zeros(1).to(DEVICE)\n",
        "\n",
        "    accs = []\n",
        "\n",
        "    for _ in range(test_epochs):\n",
        "        log = LogReg(output_size, dataset_classes_dct[dataset])\n",
        "        opt = torch.optim.Adam(log.parameters(), lr=0.01, weight_decay=0.0)\n",
        "        log.to(DEVICE)\n",
        "\n",
        "        pat_steps = 0\n",
        "        best_acc = torch.zeros(1)\n",
        "        best_acc = best_acc.to(DEVICE)\n",
        "\n",
        "        for _ in range(100):\n",
        "            log.train()\n",
        "            opt.zero_grad()\n",
        "\n",
        "            logits = log(train_embs)\n",
        "            loss = xent(logits, train_lbls)\n",
        "\n",
        "            loss.backward()\n",
        "            opt.step()\n",
        "\n",
        "        logits = log(test_embs)\n",
        "        preds = torch.argmax(logits, dim=1)\n",
        "        acc = torch.sum(preds == test_lbls).float() / test_lbls.shape[0]\n",
        "        accs.append(acc * 100)\n",
        "        print(model.__name__)\n",
        "        print(acc)\n",
        "        tot += acc\n",
        "\n",
        "    print('Average accuracy:', tot / test_epochs)\n",
        "\n",
        "    accs = torch.stack(accs)\n",
        "    print(accs.mean())\n",
        "    print(accs.std())"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data = PlanetoidSCDataset('./data', dataset, processor_type)\n",
        "print(data)"
      ],
      "metadata": {
        "id": "YAcEiQWOM81S",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1c77dce2-53f1-43c7-eebe-e84a517cbd21"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PlanetoidSCDataset()\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(data)"
      ],
      "metadata": {
        "id": "9sY6MbvQ0-Oq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b4296fc3-b921-4425-acbf-bc44a82a5ccb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PlanetoidSCDataset()\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}